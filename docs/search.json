[{"path":"index.html","id":"введение","chapter":"Введение","heading":"Введение","text":"","code":""},{"path":"index.html","id":"о-курсе","chapter":"Введение","heading":"О курсе","text":"За последние годы мир кардинально изменился. Большие языковые модели — такие как ChatGPT, Claude и Gemini — глубоко проникли практически во все сферы нашей жизни: от работы и обучения до творчества и повседневных задач. Сегодня уже сложно представить, как ещё совсем недавно мы обходились без AI-ассистентов, способных отвечать на вопросы, писать тексты, анализировать данные и помогать принимать решения.С большой вероятностью вы уже регулярно используете LLM-модели в своей работе или личных задачах. Но чаще всего это взаимодействие ограничивается уровнем обычного пользователя: задать вопрос в чате, получить ответ, иногда — уточнить или переформулировать запрос.\nВ этом курсе мы сделаем следующий шаг.Здесь вы научитесь не просто пользоваться искусственным интеллектом, а разрабатывать собственные кастомные AI-инструменты на языке R — под конкретные задачи, процессы и бизнес-сценарии. Такие инструменты могут выполнять действия, взаимодействовать с внешними сервисами, работать с данными и использовать собственную базу знаний.В ходе курса мы последовательно пройдём весь путь:начнём с основ работы с API различных LLM-провайдеров на языке R;разберёмся, как управлять запросами, контекстом и ответами моделей;построим полноценный веб-интерфейс AI-чата на базе Shiny-приложения;научим AI-ассистента пользоваться инструментами и выполнять действия;подключим его к собственной базе знаний и реализуем подходы вроде RAG.В результате вы увидите, как из набора отдельных технологий и пакетов собирается цельное AI-приложение — от первых строк кода до готового инструмента, которым можно пользоваться в реальной работе.Этот курс — не про абстрактные примеры и игрушечные демо. Он про системный подход к разработке AI-решений на языке R: архитектуру, практику, реальные сценарии и понимание того, как и зачем всё это работает.\nЕсли вам интересно выйти за рамки «чата с моделью» и начать создавать собственные интеллектуальные инструменты — вы в правильном месте.","code":""},{"path":"index.html","id":"для-кого-этот-курс","chapter":"Введение","heading":"Для кого этот курс","text":"Данный курс я не могу рекомендовать новичкам. Заниматься разработкой AI инструментов лучше имея за плечами определённый опыт написания кода на R. Поэтому не стоит начинать изучение R с данного курса, ниже я дам небольшую подборку подготовительных курсов, изучив которые можно попробовать себя в разработке пакетов.","code":""},{"path":"index.html","id":"по-поводу-поддержки-обучающихся-на-данном-курсе","chapter":"Введение","heading":"По поводу поддержки обучающихся на данном курсе","text":"Важно! Поддержки учащихся на этом курсе со стороны автора нет. Я не занимаюсь частными консультациями, тем более не консультирую студентов бесплатных курсов. Поэтому не имеет никакого смысла писать мне в личку или на почту просьбы помочь с прохождением этого, или любого другого моего бесплатного курса. Если вы столкнулись с трудностями при прохождении курса и вам нужна помощь, то все вопросы можно адресовать в следующие telegram чаты:R (язык программирования)Горячая линия RОтдельного чата со студентами непосредственно этого курса не существует, но при желании вы самостоятельно можете его организовать, и я с радостью добавлю на него ссылку.К тому же, если у вас есть вопросы по одной из лекций курса, вы можете задавать его под видео лекции на YouTube, это приветствуется, и на такие комментарии я с радостью отвечу.Буду рад любой конструктивной критике, и предложениям по улучшению курса “Язык R для разработки AI инструментов”, направлять их можно мне на почту selesnow@gmail.com. Если вы хотите выразить благодарность мне за курс, то в конце раздела описано как это можно сделать.","code":""},{"path":"index.html","id":"рекомендации-по-прохождению-курса","chapter":"Введение","heading":"Рекомендации по прохождению курса","text":"Данный курс состоит из 4 видеолекций общей продолжительностью более 2 часов. В конце каждого урока вы найдете вопросы для самопроверки, попробуйте ответить на них, для того, что бы убедиться в том, что вы усвоили материал урока.Также каждый урок содержит конспект лекции. Для достижения максимального эффекта от обучения и дальнейшего использования полученных знаний, придерживайтесь следующего алгоритма:Посмотрите полное видео лекции.Ответьте на проверочные вопросы.В дальнейшем, при разработке AI инструментов на языке R обращайтесь к конспекту нужного вам урока для поиска нужной информации и тайм кодам к видео, что бы перейти к нужной вам части видео урока.","code":""},{"path":"index.html","id":"об-авторе","chapter":"Введение","heading":"Об авторе","text":"Меня зовут Алексей Селезнёв, с 2008 года я являюсь практикующим аналитиком. На данный момент основной моей деятельностью является развитие отдела аналитики в одной из крупнейших украинских компаний - Netpeak.\nМною были разработаны такие R пакеты как: rgoogleads, rfacebookstat, timeperiodsR и некоторые другие. На данный момент написанные мной пакеты только с CRAN были установленны более 200 000 раз.Также я являюсь автором некоторых других курсов по R (ссылки на них приведу ниже), лектором академии Web Promo Experts и соавтором курса “Веб-аналитика Pro”.Веду свой авторский Telegram и YouTube канал R4marketing. Буду рад видеть вас в рядах подписчиков.Периодически публикую статью на различных интернет медиа, зачастую это Хабр и Netpeak Journal.Неоднократно выступал на профильных конференциях по аналитике и интернет маркетингу, среди которых Матемаркетинг, GoAnalytics, Analyze, eCommerce, 8P и прочие.","code":""},{"path":"index.html","id":"другие-курсы-автора","chapter":"Введение","heading":"Другие курсы автора","text":"Как я уже писал выше, помимо курса “Разработка пакетов на языке R” у меня есть ряд других бесплатных курсов:Язык R для интернет маркетинга, для начинающих, требуется бесплатная регистрацияЯзык R для пользователей Excel, для начинающихВведение в dplyr 1.0.0, средней уровень сложностиЦиклы и функционалы в языке R, средней уровень сложностиРазработка telegram ботов на языке R, высокий уровень сложностиРазработка пакетов на языке R, высокий уровень сложности","code":""},{"path":"index.html","id":"каналы-автора","chapter":"Введение","heading":"Каналы автора","text":"Если вы интересуетесь языком R, применяете его в работе, или планируете изучать, то думаю вам будут интересны мои каналы, о которых я писал выше. Буду рад видеть вас среди подписчиков:Telegram канал R4marketingYoutube канал R4marketing","code":""},{"path":"index.html","id":"программа-курса","chapter":"Введение","heading":"Программа курса","text":"Введение в разработку AI инструментов на языке R (ellmer, shinychat)Встраиваем LLM модель в Telegram ботаЯзык R как MCP сервер и MCP клиентRAG: Подключаем LLM модель к собственной базе знанийДата обновления курса: 11.01.2026facebooklinkedinwhatsapptelegramtelegramyoutube","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","text":"","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"описание-урока","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.1 Описание урока","text":"Добро пожаловать в первую главу курса. Мы начинаем путь в мир прикладного AI с позиции практиков. Сегодня большие языковые модели (LLM) перестали быть игрушкой в браузере и стали полноценным компонентом ИТ-инфраструктуры.В этом уроке мы разберем, как аналитику на языке R «приручить» эти модели для решения ежедневных задач: от автоматического анализа логов на сервере до написания кода с учетом ваших внутренних библиотек. Мы не будем тратить время на теорию нейросетей — мы сразу перейдем к сборке работающего инструмента на базе пакетов ellmer и shinychat. Главная цель этой главы — показать, что современный AI-стек доступен любому R-разработчику бесплатно и без необходимости переходить на Python.","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"видео","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.2 Видео","text":"","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"тайм-коды","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.2.1 Тайм коды","text":"00:00 — О чём это видео01:05 — Кейс практического внедрения AI в рабочие процессы06:49 — Введение в пакет ellmer08:20 — Как бесплатно сгенерировать API ключ для Gemini API09:05 — Аргументы конструктора LLM чатов11:07 — Создаём объект chat13:01 — Методы объекта chat14:30 — Отправляем первый запрос в LLM17:03 — Извлечение структурированных данных из текста24:15 — Добавляем в чат инструменты (Tool Calling)28:17 — Создаём графический интерфейс с помощью shinychat31:02 — Как дообучить модель своими данными (System Prompt)33:50 — Заключение","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"презентация","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.3 Презентация","text":"","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"конспект","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4 Конспект","text":"","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"как-мы-используем-llm-в-рабочих-процессах","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.1 Как мы используем LLM в рабочих процессах","text":"Львиная часть нашей работы заключается в разработке скриптов, которые получают данные из различных источников, обрабатывают их, и далее либо куда-то записывают, либо формируют из них сообщения или письма и рассылают. Все скрипты крутятся на сервере под Windows, и запускаются через планировщик задач. На данный момент в планировщике заданий настроено более 350 задач, запускающих разные скрипты.Для мониторинга планировщика заданий был написан бот, который запускается раз в 10 минут, и проверяет статус последнего выполнения всех настроенных нами задач. О самом боте я уже рассказывал в первой главе учебника по разработке telegram ботов.Недавно мы пошли дальше, и развернули на сервере Shiny приложение, которое запрашивает все данные по задачам из планировщика заданий Windows, и позволяет:Просматривать и фильтровать список настроенных задачПросматривать логи, т.е. Rout файлы запускаемых задачей скриптовОтправлять .Rout файл на анализ в LLM, и получать объяснение о возникновении ошибки при выполнении скрипта, и пошаговый план по устранению ошибкиПросматривать листинг скрипта запускаемого задачейОтправить код на анализ в Gemini и получить объяснение того, что этот код делаетЗапускать задачуАктивировать и деактивировать задачи в планировщике заданийТак же в приложении есть и много другого функционала, среди которых есть чат основанный на LLM, который помогает генерировать код, и исправлять ошибки.У нас достаточно много внутренних источников данных:Внутренняя самописная ERP/CRM системаВнутренняя самописная HRM системаМенеджер задачСистема финансового учётаИ ряд других внутренних источников данных.Под работу с каждым из этих источников данных у нас написаны пакеты, которые по сути являются обёрткми в которые вшиты либо SQL запросы либо вызовы API. Так вот, чат помогающий нам писать код, анализирующий Rout файл и листинг кода, о котором я писал выше, дообучен документацией по работе к нашим пакетам, и он генерирует и анализирует код и логи с использованием подробной документации к нашим внутренним пакетам, чего не может делать ни один внешний LLM чат, ни ChatGPT, ни Claude, ни Gemini.Всё это работает на базе Shiny, ellmer, shinychat и Gemini, абсолютно бесплатно. Далее я подробно расскажу о том, как всё это было разработано.","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"генерация-api-ключа-для-работы-с-llm","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.2 Генерация API ключа для работы с LLM","text":"Для работы с API любой LLM модели вам надо сгенерировать API ключ. Практически все провайдеры сейчас убрали из планов бесплатный доступ к API, единственный провайдер, у которого я нашел бесплатный тарифы это Gemini.Для генерации ключа просто зайдите в Google AI Studio, и нажмите кнопку Get API Key. В бесплатном тарифе вам доступно несколько моделей, среди которых очень неплохо себя показала Gemini 2.5 Flash.Для удобства дальнейшей работы создайте переменную среды GOOGLE_API_KEY с полученным ключём.Важное замечание по безопасности: В примерах этого урока мы будем использовать переменные среды для работы с ключами. Это удобно для обучения, но критически опасно, если вы решите опубликовать свой код. Чтобы узнать, как профессионально изолировать ключи от кода с помощью .Renviron или пакета keyring, обязательно посмотрите мой урок “Как правильно хранить секретные данные в R”.","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"работа-с-llm-в-r","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.3 Работа с LLM в R","text":"Для взаимодействия с LLM моделями в R с недавних пор появился пакет ellmer, который предоставляет вам единый интерфейс к огромному количеству провайдеров LLM, моделей. Работа с ellmer начинается с создания объекта чата через один из конструкторов chat_*().Под каждый провайдер есть свой конструктор, на данный момент ellmer поддерживает работу со следующими провайдерами LLM моделей:Anthropic’s Claude: chat_anthropic().AWS Bedrock: chat_bedrock().Azure OpenAI: chat_azure().Databricks: chat_databricks().DeepSeek: chat_deepseek().GitHub model marketplace: chat_github().Google Gemini: chat_google_gemini().Groq: chat_groq().Ollama: chat_ollama().OpenAI: chat_openai().OpenRouter: chat_openrouter().perplexity.ai: chat_perplexity().Snowflake Cortex: chat_snowflake() и chat_cortex_analyst().VLLM: chat_vllm().Установим пакет ellmer, и создадим объект chat и отправим свой первый запрос:Все функции-конструкторы чатов имеют общий набор основных аргументов:model - Имя модели, которую вы хотите использовать. Каждый провайдер предлагает различные модели, и вы можете выбрать ту, которая лучше всего подходит для вашего случая использования. Например, у OpenAI есть модели GPT-4o, gpt-4o-mini и др.system_prompt - Строка, описывающая роль или поведение чата. Это начальная подсказка, которая задает тон и стиль взаимодействия, например: «friendly assistant.»api_args - Список дополнительных аргументов, которые могут быть переданы API. Это может включать настройки специфичные для конкретного провайдера, например, температуру для генерации текста.echo - Управление выводом результата, принимает одно из значений: none, text, allОбъект chat построен на базе R6 классов, которые являются реализацией классического ООП в R, chat имеет следующие методы:chat() - Этот метод используется для отправки запроса к LLM и получения ответа в виде строки.stream() - Позволяет обрабатывать потоковые данные в реальном времени. Этот метод возвращает генератор coro, который позволяет обрабатывать ответ по мере его поступления. Это удобно для различных случаев использования, таких как запись данных в файл или отправка ответа в интерфейс Shiny.chat_async() - Асинхронная версия метода chat(). Возвращает promise, который разрешает результаты, когда ответ получен. Полезен для одновременного запуска нескольких сеансов общения или в контексте Shiny, чтобы не блокировать интерфейс.stream_async() - Асинхронная версия метода stream(). Она возвращает async generator, который позволяет обрабатывать асинхронные результаты с течением времени.chat_structured() - Используется для извлечения структурированных данных из текста или изображений. Принимает схему, определяющую, как должны быть структурированы данные. Возвращает данные в R-представлении, например в виде списка или таблицы данных.register_tool() - Регистрирует внешние функции или “инструменты”, которые чат-бот может вызывать. Позволяет настроить чат для выполнения дополнительных действий в зависимости от контекста, таких как выполнение API-запросов или манипуляции с данными.token_usage() - Возвращает информацию об использовании токенов в текущей сессии. Это полезно для оптимизации затрат на использование модели, так как позволяет следить за количеством использованных и оставшихся токенов.","code":"\npak::pak('ellmer')\nlibrary(ellmer)\n\n# API ключ\nSys.setenv(GOOGLE_API_KEY = 'ВАШ API ТОКЕН')\n\nchat <- chat_gemini(\n  system_prompt = \n    'Ты специалист по анализу данных, и разработчик на языке R. \n     В этом чате ты помогаешь генерировать код на языке R.'\n  )\n\nout <- chat$chat(\n  'Напиши мне функцию, которая по заданному \n   городу запрашивает текущу погоду из бесплатного API', \n  echo = 'none'\n  )"},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"как-с-помощью-llm-извлекать-структурированные-данные-из-текста","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.4 Как с помощью LLM извлекать структурированные данные из текста","text":"Большинство LLM моделей обладают функцией извлечения структурированных данных их текста. С помощью чего вы можете:быстро обрабатывать полнотекстовые анкеты извлекая из них нужные данныеопределить настроение комментариевклассифицировать статьи по тематикамДа и в целом можно найти огромное количество вариантов применения этой функции. Реализуется она с помощью метода $chat_structured(), в который вам необходимо передать текст, из которого планируете извлечь структурированные данные, и описание структуры, которую ы хотите извлечь из текста.Т.е. изначально вам необходимо создать объект описания структуры с помощью функции type_object(), внутри которого вы описываете каждый отдельный извлекаемый элемент структуры с помощью других функций семейства type_*(), все типы данных можно условно поделить на 3 категории:Скаляры представляют собой отдельные значения, которые бывают пяти типов: type_boolean(), type_integer(), type_number(), type_string() и type_enum(), представляющие собой отдельные логические, целочисленные, двойные, строковые и факторные значения соответственно.Массивы представляют любое количество значений одного типа и создаются с помощью type_array(). Вы всегда должны указывать item аргумент, который определяет тип каждого отдельного элемента. Массивы скаляров очень похожи на атомарные векторы R.Объекты представляют собой набор именованных значений и создаются с помощью type_object(). Объекты могут содержать любое количество скаляров, массивов и других объектов. Они похожи на именованные списки в R.","code":"\n## описание структуры\npersonal_data_str <- type_object(\n  age  = type_integer('Возраст в годах, целое число'),\n  name = type_string('Имя'),\n  job  = type_string('Занимаемая на работе должность')\n)\n\n## извлекаем информацию\ntext <- \"\nПривет, меня зовут Алексей, мне 40 лет, с 2016 года занимаю должность \nруководителя отдела аналитики.\n\"\npersonal_data <- chat$chat_structured(text, type = personal_data_str)\n\n# классификация настроения комментария\ntext <- \"\nКупленный товар работает отлично, к нему никаких претензий нет, \nно обслуживание клиентов было ужасным. \nЯ, вероятно, больше не буду у них покупать.\n\"\ntype_sentiment <- type_object(\n  \"Извлеки оценки настроений заданного текста. Сумма оценок настроений должна быть равна 1.\",\n  positive_score = type_number(\"Положительная оценка, число от 0.0 до 1.0.\"),\n  negative_score = type_number(\"Отрицательная оценка, число от 0.0 до 1.0.\"),\n  neutral_score = type_number(\"Нейтральная оценка, число от 0.0 до 1.0.\")\n)\n\nstr(chat$chat_structured(text, type = type_sentiment))"},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"добавляем-в-чат-инструменты","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.5 Добавляем в чат инструменты","text":"Метод $register_tool() позволяет вам встраивать в ваш чат дополнительные инструменты, например интеграцию с любыми другими API, или в целом описать любой другой инструментарий посредствам создания обычных R функций, которые вы добавите в чат. Например, ни одна LLM модель не может получить данные в реальном времени, она не знает даже текущего времени, не может дать вам информацию о текущей погоде в каком либо городе. Но вы можете обучить этому свой чат, добавив в него функции:В этом примере мы написали функцию get_current_time(), которая получает текущее время по указанному часовому поясу. Далее мы добавили эту функцию в чат с помощью метода register_tool(), и функции tool(), которая позволяет вам дать описание того что передаваемая в чат функция делает, и описать каждый её аргумент.","code":"\nchat <- chat_gemini()\n\nchat$chat('Какое текущее время сейчас по Киеву?')\n\n#' Gets the current time in the given time zone.\n#'\n#' @param tz The time zone to get the current time in.\n#' @return The current time in the given time zone.\nget_current_time <- function(tz = \"UTC\") {\n  format(Sys.time(), tz = tz, usetz = TRUE)\n}\n\nchat$register_tool(tool(\n  get_current_time,\n  name = \"get_current_time\",\n  description = \"Получить текущее время в указанном часовом поясе.\",\n  arguments = list(\n    tz = type_string(\n      \"Часовой пояс. По умолчанию `\\\"UTC\\\"`.\",\n      required = FALSE\n    )\n  )\n))\n\nchat$chat('Какое текущее время сейчас по Киеву?')"},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"создаём-shiny-интерфейс-для-чата","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.6 Создаём Shiny интерфейс для чата","text":"С функционалом пакета ellmer разобрались, теперь перейдём к тому, как упаковать чат в графический интерфейс. Для этого проще всего использовать пакет shinychat.Представленный выше код Shiny приложения запустит интерфейс для вашего чата.","code":"\nlibrary(shiny)\nlibrary(shinychat)\n\nui <- bslib::page_fluid(\n  chat_ui(\"chat\")\n)\n\nserver <- function(input, output, session) {\n  chat <- ellmer::chat_gemini(\n    system_prompt = \"Ты специалист по разработке кода и анализу данных на языке R\"\n    )\n  \n  observeEvent(input$chat_user_input, {\n    stream <- chat$stream_async(input$chat_user_input)\n    chat_append(\"chat\", stream)\n  })\n}\n\nshinyApp(ui, server)"},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"как-дообучить-модель-своими-данными","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.7 Как дообучить модель своими данными","text":"Основным аргументом позволяющим вам дообучать модель собственными данными, например документацией к вашим пакетам, или просто описанием каких либо ваших рабочих процессов источников данных, или чего угодно, является system_prompt.Помимо обычной строки вы можете скормить ему довольно большой .md файл, в котором будет вся необходимая для обучения информация. В своём приложении я дал базовое описание:какие внутренние источники данных у нас естьопределение того какой пакет предназначен для работы с каким источникомописание всех функций, всех аргументов, и всех возвращаемых каждой функцией пакета полейТ.е. мой md файл выглядит примерно так:Далее вы передаёте этот .md файл в аргумент system_prompt:После чего ваш чат будет обучен на основе ваших данных, и будет помогать как в генерации кода, так и в исправлении ошибок при запуске созданных ранее скриптов.","code":"В этом чате ты выполняешь несколько фукнций:\n\n1. Анализируешь логи выполнения скриптом читая Rout файлы, даёшь объяснения ошибки и пошаговый план её исправления\n2. Помогаешь генерировать код на основе внутренних корпоративных пакетов\n\n# Внутренние источники данных\nТут базово описаны внутренние истоники данных\n\n# Соответвие пакета и источника данных\nПрописываем какой пакет предназначен для работы с каждым из источников данных\n\n# Документация к пакетам\nНиже приведена подробная документация к корпоративным пакетам, используй эту документацию для генерации кода и анализа Rout файлов.\n\n## Название пакета\nРаботает с источником 1\n\n## Функции пакета\n\n* название фукнции 1 - описание\n  * аргументы\n    * аргумент 1 - описание\n    * аргумент 2 - описание\n  * поля которые возвращает функция\n    * поле 1 - описание\n    * поле 2 - описание\n* название фукнции 2 - описание\n  * аргументы\n    * аргумент 1 - описание\n    * аргумент 2 - описание\n  * поля которые возвращает функция\n    * поле 1 - описание\n    * поле 2 - описание\nchat <- ellmer::chat_gemini(\n    system_prompt = interpolate_file(path = 'system_prompt.md')\n)"},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"заключение","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.4.8 Заключение","text":"Мы заложили фундамент: наш AI-чат понимает документацию, умеет использовать инструменты R и возвращает данные в удобном нам формате. Но интерфейс Shiny требует открытого браузера и активной сессии.В следующей главе мы сделаем этот интеллект мобильным. Мы возьмем наработки из этой главы и «упакуем» их в Telegram-бота. Это позволит вам получать анализ серверных ошибок или генерировать код прямо со смартфона, находясь в дороге или на совещании. Мы разберем, как подружить асинхронность ellmer с событийной моделью Telegram-бота.","code":""},{"path":"введение-в-разработку-ai-инструментов-на-языке-r-ellmer-shinychat.html","id":"вопросы-для-самопроверки","chapter":"Глава 1 Введение в разработку AI инструментов на языке R (ellmer, shinychat)","heading":"1.5 Вопросы для самопроверки","text":"В чем главное преимущество пакета ellmer перед прямым обращением к API конкретной модели?\n\nПакет ellmer предоставляет единый интерфейс (унифицированные функции и методы) для работы с десятками разных провайдеров (Gemini, Claude, OpenAI, Ollama). Это позволяет переключаться между моделями разных компаний, практически не меняя основной код вашего приложения.\nКакой метод объекта chat следует использовать, если вам нужно получить результат анализа текста в виде именованного списка R, а не просто строки?\n\nДля этого используется метод $chat_structured(). Он заставляет модель возвращать ответ в строго заданном формате (JSON под капотом), который ellmer автоматически преобразует в привычные объекты R (списки или векторы).\nЗачем нужен метод $register_tool()?\n\nЭтот метод позволяет расширить возможности модели, предоставив ей доступ к вашим собственным функциям на языке R. С его помощью модель может получать актуальные данные из интернета, обращаться к базам данных или выполнять вычисления, которые ей недоступны «из коробки».\nКак “обучить” модель специфике вашей компании, если вы не занимаетесь Fine-tuning (дообучением весов)?\n\nЭто делается через System Prompt. Вы можете передать в этот аргумент (в том числе загрузив из .md файла) подробную инструкцию, описание ваших бизнес-процессов и документацию к внутренним пакетам. Модель будет использовать этот контекст при каждом ответе.\nКакие три категории типов данных используются при извлечении структурированной информации методом $chat_structured()?\nСкаляры (type_string, type_integer, type_number, type_boolean, type_enum) — одиночные значения.\nМассивы (type_array) — списки однотипных значений (аналог векторов).\nОбъекты (type_object) — наборы именованных полей, которые могут содержать в себе скаляры, массивы и другие объекты (аналог именованных списков).\nСкаляры (type_string, type_integer, type_number, type_boolean, type_enum) — одиночные значения.Массивы (type_array) — списки однотипных значений (аналог векторов).Объекты (type_object) — наборы именованных полей, которые могут содержать в себе скаляры, массивы и другие объекты (аналог именованных списков).","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"встраиваем-llm-модель-в-telegram-бота","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"Глава 2 Встраиваем LLM модель в Telegram бота","text":"В первой главе мы создали интеллект и дали ему «лицо» в виде Shiny-приложения. Но настоящий ассистент аналитика должен быть доступен в один клик прямо в смартфоне.В этой главе мы переместим наш AI-движок в Telegram. Главный вызов здесь — многопользовательская среда. Мы разберем, как сделать так, чтобы бот не путал контекст диалогов разных людей и помнил историю переписки даже после перезагрузки сервера. Мы превратим Telegram из простого мессенджера в полноценную точку доступа к вашему кастомному ИИ.Если вы не знакомы с принципами разработки telegram ботов на языке R то можете пройти курс “Разработка telegram ботов на языке R”.","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"видео-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.1 Видео","text":"","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"тайм-коды-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.1.1 Тайм коды","text":"00:00 — О чём это видео00:46 — Генерация API ключа02:20 — Введение в пакет ellmer03:20 — Создаём объект чата06:44 — Извлечение структурированных данных из текста10:35 — Классификация текста с помощью LLM моделей13:53 — Интеграция LLM моделей со сторонними API18:33 — Как интегрировать LLM модель в Telegram-бота23:27 — Как сохранять состояние чатов (Session Handling)26:24 — Как дообучить бота своими данными (System Prompt)28:58 — Заключение","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"презентация-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.2 Презентация","text":"","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"конспект-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.3 Конспект","text":"","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"интеграция-llm-модели-в-бот","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.3.1 Интеграция LLM модели в бот","text":"Давайте разберёмся с тем, как добавить весь описанный в первой главе функционал в Telegram бота. Основная сложность с которой вы можете столкнуться это то, как хранить одновременно отдельные объекты chat для каждого отдельного чата в Telegram. Т.к. если вы просто создадите один объект чата, и в него будут прилетать сообщения из разных telegram чатов, и разных пользователей, то контекст чата будет запутан, и соответвенно модель в чате не сможет осмысленно связать все входящие сообщения, и качество ответов будет оставлять лучшего. Поэтому одним из вариантов хранения информация о разных чатах является создание списка, в котором в виде отдельных элементов будут хранится разные объекты чата для разных telegram чатов, название каждого элемента списка будет соответствовать идентификатору чата в telegram.Простейший пример кода, для интеграции LLM модели в telegram бот выглядит так:Безопасность прежде всего: Обратите внимание на функцию bot_token(). Никогда не вставляйте токен бота строкой прямо в код. Самый надежный способ — прописать его в файле .Renviron под именем R_TELEGRAM_BOT_TEST. Подробнее о том, почему это важно и как это настроить, смотрите в уроке “Работа с секретными данными на языке R”.Что делает этот код и как он устроен:\nВ этом примере мы создаём Telegram-бота, который использует LLM-модель через пакет ellmer и умеет вести отдельные диалоги с каждым пользователем, сохраняя контекст общения.Инициализация переменной sessions:\nВ начале мы создаём глобальный список sessions, куда будут добавляться объекты чата chat_google_gemini для каждого Telegram-пользователя. Ключом в этом списке будет ID Telegram-чата (chat_id). Это позволяет хранить независимый LLM-контекст для каждого пользователя.Инициализация переменной sessions:\nВ начале мы создаём глобальный список sessions, куда будут добавляться объекты чата chat_google_gemini для каждого Telegram-пользователя. Ключом в этом списке будет ID Telegram-чата (chat_id). Это позволяет хранить независимый LLM-контекст для каждого пользователя.Обработчик команды /start:\nКогда пользователь впервые запускает бота командой /start, срабатывает start_handler.\nВ нём:Обработчик команды /start:\nКогда пользователь впервые запускает бота командой /start, срабатывает start_handler.\nВ нём:Получается chat_id пользователя.Создаётся новый объект chat_google_gemini с заданным system_prompt. В prompt задаётся роль модели: помощник по разработке на языке R.Так же мы ограничиваем максимальную длинну ответа от LLM передав через api_args параметр max_output_tokens, поскольку в telegram есть лимит на длинну сообщения в 4096 символов. api_args - аргумент, который позволяет передавать в запросе различные параметры, такие как температура, максимальная длинна ответа и так далее, набор этих параметров у каждой модели свой.Этот объект сохраняется в список sessions, где ключом служит chat_id.Пользователю отправляется приветственное сообщение.Обработчик текстовых сообщений:\nЭтот обработчик вызывается каждый раз, когда пользователь пишет что-то в чат. Внутри:Получаем chat_id и по нему — соответствующий объект из sessions.Если объект чата не найден (например, пользователь не вызвал /start), то бот просит сначала это сделать.Если всё ок — берём текст сообщения пользователя и отправляем его в LLM через chat$chat().Полученный ответ от модели возвращается пользователю в Telegram.Запуск бота:\nДалее создаётся объект Updater с токеном бота, к нему добавляются два обработчика: для команды /start и для всех обычных сообщений.\nЗатем вызывается start_polling(), чтобы бот начал слушать входящие сообщения.В чём суть логики?\nГлавное в этом подходе — поддержка сессий для каждого пользователя. Благодаря списку sessions мы можем вести независимые диалоги с разными пользователями одновременно. Каждый chat_gemini живёт в своей “ячейке” и помнит контекст диалога. Это особенно важно для того, чтобы LLM не путала запросы между разными пользователями, и могла давать максимально точные и уместные ответы.Обратите внимание, что в системном промпте я отдельно указал “Твои ответы должны занимать не более 3000 символов.”, это сделано для того, что полученный от модели ответ помещался в одно сообщение telegram, которое на данный момент имеет лимит в 4096 символов.Приведённый выше бот будет хранить все чаты в рамках одной своей сессии, после перезапуска все чаты будут удалены из его памяти. Если вам нужен бот, который будет хранить информацию о всех чатах между сессиями то объекты чата надо хранить в виде локальных rds файлов. Для реализации надо:Написать функции для сохранения и записи объектов чатов в RDS файлыДоработать функции start_handler() и message_handler() так, что бы они читали и сохраняли объекты чата в отдельные RDS файлы.Сохраним код функций для работы с RDS файлами в отдельный session_func.R файл.И доработаем код бота:Как изменилась логика работы бота\nВ этой версии бота добавлена долговременная память — теперь каждый чат сохраняется в отдельный .rds-файл, а значит, бот не забывает переписку после перезапуска.Основные изменения:Загрузка и сохранение сессий:\nВместо хранения объектов chat_gemini в оперативной памяти (в списке sessions) теперь используется файловая система. Добавлены функции load_chat() и save_chat(), которые читают и записывают объекты чата в .rds-файлы (по одному на каждый chat_id). Эти функции подключаются из внешнего файла session_func.R.Загрузка и сохранение сессий:\nВместо хранения объектов chat_gemini в оперативной памяти (в списке sessions) теперь используется файловая система. Добавлены функции load_chat() и save_chat(), которые читают и записывают объекты чата в .rds-файлы (по одному на каждый chat_id). Эти функции подключаются из внешнего файла session_func.R.Обновлён start_handler():\nТеперь при вызове /start бот сначала проверяет, есть ли сохранённый .rds-файл сессии для данного пользователя. Если есть — загружает его. Если нет — создаёт новый чат-объект и сохраняет его.Обновлён start_handler():\nТеперь при вызове /start бот сначала проверяет, есть ли сохранённый .rds-файл сессии для данного пользователя. Если есть — загружает его. Если нет — создаёт новый чат-объект и сохраняет его.Обновлён message_handler():\nЗдесь всё то же, что и раньше, но теперь после каждого запроса дополнительно сохраняется обновлённый объект чата обратно в .rds. Это важно, чтобы вся история общения сохранялась между сессиями.Обновлён message_handler():\nЗдесь всё то же, что и раньше, но теперь после каждого запроса дополнительно сохраняется обновлённый объект чата обратно в .rds. Это важно, чтобы вся история общения сохранялась между сессиями.Таким образом, теперь бот умеет “помнить”, о чём вы говорили с ним ранее, даже если он был выключен или перезапущен — очень полезно для долгосрочного взаимодействия с пользователями.","code":"\nlibrary(telegram.bot)\nlibrary(ellmer)\n\n# Создаём глобальную переменную для хранения сессий\n# в которую будут добавляться новые чаты\nsessions <- list()\n\n# Handler для команды /start\nstart_handler <- function(bot, update) {\n  \n  chat_id <- update$message$chat$id\n  \n  # Создаём новый чат-объект для пользователя с уникальным чатом\n  chat <- chat_google_gemini(\n    system_prompt = \"\n        Ты специалист по разработке кода и анализу данных на языке R, \n        в этом чате помогаешь с разработкой кода на R. Твои ответы должны занимать не более 3000 символов.\n    \",\n    api_args = list(\n      generation_config = list(\n        max_output_tokens = 1500\n      )\n    )\n  )\n  \n  # Сохраняем чат-объект в глобальной переменной sessions\n  sessions[[as.character(chat_id)]] <<- chat\n  \n  bot$sendMessage(chat_id = chat_id, text = \"Здравствуйте, чем могу вам помочь?\")\n  \n}\n\n# Handler для текстовых сообщений\nmessage_handler <- function(bot, update) {\n  \n  chat_id <- update$message$chat$id\n  \n  # Получаем чат-объект для пользователя\n  chat <- sessions[[as.character(chat_id)]]\n\n  # Если чат не найден то просим выполнить команду start для его запуска\n  if (is.null(chat)) {\n    bot$sendMessage(chat_id = chat_id, text = \"Используйте /start для начала AI чата.\")\n    return(NULL)\n  }\n  \n  # текст запроса\n  user_message <- update$message$text\n  \n  # отправляем запрос пользователя в LLM\n  response <- chat$chat(user_message, echo = FALSE)\n  \n  # отправляем в чат полученный от LLM ответ\n  bot$sendMessage(\n    chat_id = chat_id, \n    text = response,\n    parse_mode = 'markdown'\n  )\n\n}\n\n# Инициализируем бот и добавляем обработчики\nupdater <- Updater(bot_token('TEST'))\n\n# Обработчики\nh_start <- CommandHandler(\"start\", start_handler)\nh_msgs  <- MessageHandler(message_handler, MessageFilters$text)\n\nupdater <- updater + h_start + h_msgs\n\n# Запускаем бота\nupdater$start_polling()\nsave_chat <- function(chat_id, chat_object) {\n  file_path <- file.path(\"chat_sessions\", paste0(chat_id, \".rds\"))\n  saveRDS(chat_object, file_path)\n}\n\nload_chat <- function(chat_id) {\n  file_path <- file.path(\"chat_sessions\", paste0(chat_id, \".rds\"))\n  if (file.exists(file_path)) {\n    return(readRDS(file_path))\n  } else {\n    return(NULL)\n  }\n}\nlibrary(telegram.bot)\nlibrary(ellmer)\n\n# Загрузка функций чтения объектов чата\nsource('session_func.R')\n\n# Handler для команды /start\nstart_handler <- function(bot, update) {\n  \n  chat_id <- update$message$chat$id\n  \n  # Проверяем был ли ранее создан чат\n  chat <- load_chat(chat_id)\n  # Создаём новый чат-объект для пользователя с уникальным чатом\n  if (is.null(chat)) {\n    chat <- chat_gemini(\n      system_prompt = paste(readLines(here::here('system_prompt.md')), collapse = \"\\n\"),\n      api_args = list(\n        generation_config = list(\n          max_output_tokens = 1500\n        )\n      )\n    )\n    \n    # сохраняем объект чата\n    save_chat(chat_id, chat)\n    \n  }\n  \n  bot$sendMessage(chat_id = chat_id, text = \"Здравствуйте, чем могу вам помочь?\")\n  \n}\n\n# Handler для текстовых сообщений\nmessage_handler <- function(bot, update) {\n  \n  chat_id <- update$message$chat$id\n  \n  # Получаем чат-объект для пользователя\n  chat <- load_chat(chat_id)\n  \n  # Если чат не найден то просим выполнить команду start для его запуска\n  if (is.null(chat)) {\n    bot$sendMessage(chat_id = chat_id, text = \"Используйте /start для начала AI чата.\")\n    return(NULL)\n  }\n  \n  # текст запроса\n  user_message <- update$message$text\n  \n  # отправляем запрос пользователя в LLM\n  response <- chat$chat(user_message, echo = FALSE)\n  \n  # сохраняем объект чата\n  save_chat(chat_id, chat)\n  \n  # отправляем в чат полученный от LLM ответ\n  bot$sendMessage(\n    chat_id = chat_id, \n    text = response,\n    parse_mode = 'markdown'\n  )\n  \n}\n\n# Инициализируем бот и добавляем обработчики\nupdater <- Updater(bot_token('TEST'))\n\n# Обработчики\nh_start <- CommandHandler(\"start\", start_handler)\nh_msgs  <- MessageHandler(message_handler, MessageFilters$text)\n\nupdater <- updater + h_start + h_msgs\n\n# Запускаем бота\nupdater$start_polling()"},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"конвертация-ответа-от-llm-в-telegram-markdownv2","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.3.2 Конвертация ответа от LLM в Telegram MarkdownV2","text":"Описанный выше код будет вполне корректно работать при простых ответах от LLM. Но есть одна проблема, с которой сталкиваются почти все разработчики, которые пытаются переслать полученный от LLM ответ сразу в telegram - Bad Request: parse entities. Дело в том, что все LLM отдают свой ответ в разметке GitHub-flavored Markdown (GFM), а telegram требуеют свою Markdown разметку, причём довольно строго относится к ошибкам в ней. У Telegram строгие правила экранирования множества символов и немного иной набор допустимых конструкций.Один из вариантов решения “в лоб” - просто поменять разметку в parse_mode на HTML, она обычно не возвращает ошибку Bad Request: parse entities, но и само сообщение полученное в Telegram будет выглядеть сырым, с элементами разметки, смотреться это будет откровенно говоря не презентабельно.Писать собственный конвертер GFM -> Telegram MarkdownV2 задача достаточно непростая, но можно использовать готовое решение, правда написанное на Python - модуль telegramify-markdown.Для начала установите пакет reticulate, который позволяет выполнять python код внутри R. Далее, если у вас не установлен Python запустите процесс установки:Далее мы можем написать R обёртку:И использовать её для конвертации полученного от LLM ответа в корректный Telegram MarkdownV2 внутри нашего message_handler:Функция markdown_to_telegram() делает следующее:\n1. Парсит Markdown в AST (Abstract Syntax Tree) с помощью библиотеки mistune\n2. Рендерит каждый элемент отдельно: заголовки, ссылки, списки, таблицы, код\n3. Сохраняет контекст форматирования: отслеживает, находимся ли мы внутри жирного текста, курсива и т.д.\n4. Правильно обрабатывает вложенности: например, bold italic bold bold\n5. Экранирует только нужные символы: внутри кода не экранирует, внутри ссылок экранирует по-особому\n6. Обрабатывает Latex: распознает формулы \\(...\\) и \\[...\\]\n7. Работает с таблицами: конвертирует их в читаемый формат\n8. Обрабатывает цитаты: правильно форматирует > блоки\n9. Тестирована на тысячах реальных случаевТ.е. является полноценным, стабильно работающим, и постоянно обновляющимся конвертером полученных от LLM ответов в корректный Telegram MarkdownV2.","code":"\ninstall.packages('reticulate')\n\n# Импортировать модуль\nlibrary(reticulate)\n\n# установка python\ninstall_miniconda()\n\n# Создать окружение и установить пакет\nvirtualenv_create(\"r-telegram-env\")\nuse_virtualenv(\"r-telegram-env\")\npy_install(\"telegramify-markdown\")\n# Импортировать модуль\ntelegramify <- import(\"telegramify_markdown\")\n\n# Функция на R\nmarkdown_to_telegram <- function(markdown_text, \n                                 max_line_length = NULL,\n                                 normalize_whitespace = FALSE) {\n  converted <- telegramify$markdownify(\n    markdown_text,\n    max_line_length = max_line_length,\n    normalize_whitespace = normalize_whitespace\n  )\n  return(converted)\n}\n# отправляем запрос пользователя в LLM\nresponse <- chat$chat(user_message, echo = FALSE)\n\n# конвертируем в Telegram MarkdownV2\ntg_response <- markdown_to_telegram(response)\n  \n# отправляем в чат полученный от LLM ответ\nbot$sendMessage(\n  chat_id = chat_id, \n  text = tg_response,\n  parse_mode = 'MarkdownV2'\n)"},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"заключение-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.4 Заключение","text":"Теперь ваш бот — это не просто скрипт, а интеллектуальный собеседник с “долгой памятью”. Мы научили его узнавать пользователей и сохранять контекст в RDS-файлы.Однако пока наш бот заперт внутри своих знаний и того, что мы передали ему в промпте. Он не может заглянуть в вашу базу данных или прочитать свежую Google-таблицу. В следующей главе мы сотрем эти границы. Мы разберем протокол MCP (Model Context Protocol) и научим бота выходить во “внешний мир”: выполнять ваши функции на R, ходить по API и работать с реальными файлами на сервере. Время дать вашему ассистенту настоящие руки!","code":""},{"path":"встраиваем-llm-модель-в-telegram-бота.html","id":"вопросы-для-самопроверки-1","chapter":"Глава 2 Встраиваем LLM модель в Telegram бота","heading":"2.5 Вопросы для самопроверки","text":"Почему нельзя использовать один глобальный объект chat для всех пользователей Telegram-бота?\n\nКонтексты сообщений от разных людей перемешаются в одной сессии. Модель будет пытаться связать вопрос Пользователя Б с ответом, который она только что дала Пользователю А, что приведет к путанице и потере логики диалога.\nКакую роль играет ID чата (chat_id) в архитектуре хранения сессий?\n\nОн служит уникальным ключом. По этому идентификатору бот понимает, какой именно объект чата (из списка в памяти или из файла RDS) нужно подтянуть для текущего собеседника.\nЗачем в системном промпте бота рекомендуется ограничивать длину ответа (например, до 3000 символов)?\n\nЭто техническая подстраховка. Лимит одного сообщения в Telegram — 4096 символов. Если модель сгенерирует слишком длинный текст (например, большой кусок кода с пояснениями), бот выдаст ошибку при попытке отправить такое сообщение.\nВ чем преимущество хранения объектов чата в RDS-файлах по сравнению со списком в оперативной памяти?\n\nЭто обеспечивает «персистентность» (постоянство) данных. Если сервер или скрипт перезагрузится, данные в оперативной памяти (список sessions) сотрутся, а файлы на диске останутся. Бот сможет продолжить диалог с того же места.\nКакая функция в пакете ellmer отвечает за отправку сообщения и получение ответа?\n\nМетод $chat() (например, chat$chat(user_message)). Он отправляет текст провайдеру LLM и возвращает строку с ответом.\nЗачем ограничивать длину ответа и как это сделать надежнее всего?\n\nЛимит сообщения в Telegram — 4096 символов. Надежнее всего использовать комбинацию: просить модель быть краткой в system_prompt и выставлять жесткий лимит через api_args = list(max_output_tokens = 1500).\n","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"язык-r-как-mcp-сервер-и-mcp-клиент","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"Глава 3 Язык R как MCP сервер и MCP клиент","text":"В предыдущих главах мы научили нашего бота говорить и помнить контекст. Но до сих пор он оставался лишь “советчиком”, запертым внутри диалогового окна. Чтобы превратить ИИ в настоящего цифрового сотрудника, нам нужно дать ему доступ к нашим инструментам: базам данных, API и локальным скриптам.В этой главе мы освоим Model Context Protocol (MCP) — открытый стандарт, который позволяет языковым моделям бесшовно подключаться к вашим R-функциям. Мы пройдем путь от написания своего MCP-сервера на языке R до подключения его к профессиональным инструментам вроде Claude Desktop. Вы увидите, как превратить обычную библиотеку для работы с Google Таблицами в мощный AI-инструмент, который выполняет ваши команды в реальном времени.","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"видео-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.1 Видео","text":"","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"тайм-коды-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.1.1 Тайм коды","text":"00:00 — О чём это видео01:50 — Что такое MCP сервер: концепция и протокол03:10 — Пишем свой MCP сервер на языке R (пакет mcptools)13:03 — Использование R в качестве MCP клиента20:24 — Подключение R-сервера к Claude Desktop23:10 — Работа с активными R сессиями через пакет btw27:09 — Заключение и итоги","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"презентация-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.2 Презентация","text":"","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"конспект-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3 Конспект","text":"","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"что-такое-mcp-сервер","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3.1 Что такое MCP сервер","text":"MCP-сервер — это сервис, который предоставляет языковым моделям (типа ChatGPT или Claude) доступ к функциям и данным на вашей стороне. Модель подключается к серверу через единый стандартный протокол и получает «инструменты»: функции, API-эндпоинты, доступ к файлам, базе данных, Google Sheets и т.д.По сути: MCP-сервер = адаптер между вашим кодом и ИИ, который говорит с моделью на понятном ей стандартизированном языке.Это способ научить ИИ пользоваться вашими функциями, но не через обычный function calling (как внутри одного запроса), а через универсальный сервер, к которому могут подключаться разные приложения — Claude, VSCode плагины, ellmer, ваш Shiny, боты и т.д.","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"язык-r-как-mcp-сервер","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3.2 Язык R как MCP сервер","text":"Используя пакет mcptools язык R может выполнять роль и MCP сервера, и MCP клиента.MCP сервер на R должен состоять из следующих компонентов:Функции на языке R;Аннотации к этим функциям, т.е. их описание;Скрипт должен возвращать список объектов, созданных с помощью ellmer::tool().Ниже я приведу небольшой пример написанного на языке R MCP сервера для работы с Google Sheets API.Если вы ранее не работали с Google Sheets API на языке R то можете обратиться к статье “Как работать с API Google Таблиц (Google Sheets API v4) на языке R с помощью нового пакета googlesheets4”.Сохраняем код нашего MCP сервера в файл gs-tools.R.Код этого MCP сервера, как я и писал выше, состоит из 3 блоков:Создание R функций:\n\n.gs_init_demo() - создание Google таблицы\n\n\n.gs_read() - чтение google таблицы\n\n\n.gs_append_contact() - добавить строку в google таблицу\n\n\n.gs_info() - чтение метаданных google таблицы\n\n\n.gs_show_env() - чтение переменной среды\n\n\nmsum() / mmax() - арифметические функции\n\n\n.gs_init_demo() - создание Google таблицы\n\n.gs_read() - чтение google таблицы\n\n.gs_append_contact() - добавить строку в google таблицу\n\n.gs_info() - чтение метаданных google таблицы\n\n.gs_show_env() - чтение переменной среды\n\nmsum() / mmax() - арифметические функции\nОписание функций:\n\nTool_gs_init\n\n\nTool_gs_read\n\n\nTool_gs_append_contact\n\n\nTool_gs_info\n\n\nTool_show_env\n\n\nTool_sum\n\n\nTool_max\n\n\nTool_gs_init\n\nTool_gs_read\n\nTool_gs_append_contact\n\nTool_gs_info\n\nTool_show_env\n\nTool_sum\n\nTool_max\nВозвращает список инструментовЕсть несколько правил, которые необходимо помнить при написании своего MCP сервера:Все функции должны возвращать JSON объекты, поэтому у меня все возвращаемые объекты завёрнуты в return(jsonlite::toJSON(obj, pretty = TRUE, auto_unbox = TRUE))Если внутри функции вам необходимо использовать какие-то пакеты, то прописывайте полный путь к этой функции, т.е. package::function() а не просто function().В конце скрипта должен возвращаться список созданных инструментов.При описании своих функций с помощью ellmer::tool() вам необходимо передать следующие аргументы:Сам объект функции, которую описываетеДать её имя, которое будет использовать модельЗадать подробное описание функции, именно по описание модель из контекста промпта будет понимать какую функцию ей необходимо вызвать.Отдельным списком дать описание каждого аргумента функции, для обозначения того, какого типа данные необходимо передавать в каждый из аргументов функции используйте семейство функций ellmer::type_*(), которые можно разделить на 3 категории:\nСкаляры представляют собой отдельные значения, которые бывают пяти типов: type_boolean(), type_integer(), type_number(), type_string() и type_enum(), представляющие собой отдельные логические, целочисленные, двойные, строковые и факторные значения соответственно.\nМассивы представляют любое количество значений одного типа и создаются с помощью type_array(). Вы всегда должны указывать item аргумент, который определяет тип каждого отдельного элемента. Массивы скаляров очень похожи на атомарные векторы R. Если необходимо передавать вектор какого то определённого типа, то необходимо использовать сочетание функций, например если аргумент требует передать целочисленный вектор то описать это можно так: type_array(type_integer()).\nОбъекты представляют собой набор именованных значений и создаются с помощью type_object(). Объекты могут содержать любое количество скаляров, массивов и других объектов. Они похожи на именованные списки в R.\nСкаляры представляют собой отдельные значения, которые бывают пяти типов: type_boolean(), type_integer(), type_number(), type_string() и type_enum(), представляющие собой отдельные логические, целочисленные, двойные, строковые и факторные значения соответственно.Массивы представляют любое количество значений одного типа и создаются с помощью type_array(). Вы всегда должны указывать item аргумент, который определяет тип каждого отдельного элемента. Массивы скаляров очень похожи на атомарные векторы R. Если необходимо передавать вектор какого то определённого типа, то необходимо использовать сочетание функций, например если аргумент требует передать целочисленный вектор то описать это можно так: type_array(type_integer()).Объекты представляют собой набор именованных значений и создаются с помощью type_object(). Объекты могут содержать любое количество скаляров, массивов и других объектов. Они похожи на именованные списки в R.Для того, чтобы подключить MCP сервер к какому либо клиенту, будь то ellmer или Claude Desktop, необходимо прописать его конфиг в виде json файла:mcpServers это объект верхнего уровня, каждый его дочерний элемент является отдельным подключенным MCP сервером.r-gsheets - заданное нами имя для MCP сервера, который мы написали.command - каждый MCP сервер запускает какую либо команду, в нашем случае MCP сервер запускается через утилиту Rscript.args - сюда мы передаём аргументы командной строки, в нашем случае для выполнения R кода прописывается \"-e\", и далее сама команда запуска MCP сервера \"mcptools::mcp_server(tools='G:/GS_MCP/gs-tools.R')\", т.е. мы запускаем MCP сервер, который считывает инструменты из созданного нами файла gs-tools.R.env - позволяет передать в MCP сервер переменные среды, в моём случае:\nGOOGLE_SERVICE_ACCOUNT_JSON - передаю путь к ключу сервисного аккаунта Google, под которым проходит авторизация в Google Sheets.\nR_LIBS_USER - отдельно передаю путь к моей пользовательской библиотеке пакетов, получить путь можно через .libPaths(). Если не задавать эту переменную то в моём случае MCP сервер на windows вообще не запускался, потому что не понимал откуда подгружать необходимые пакеты.\nGOOGLE_SERVICE_ACCOUNT_JSON - передаю путь к ключу сервисного аккаунта Google, под которым проходит авторизация в Google Sheets.R_LIBS_USER - отдельно передаю путь к моей пользовательской библиотеке пакетов, получить путь можно через .libPaths(). Если не задавать эту переменную то в моём случае MCP сервер на windows вообще не запускался, потому что не понимал откуда подгружать необходимые пакеты.","code":"\n# gs-tools.R ----------------------------------------------\nsuppressPackageStartupMessages({\n  library(ellmer)\n  library(googlesheets4)\n  library(tibble)\n})\n\n\n# Создание R функций\n\n# 1) Создать демо‑таблицу ------------------------------------------------------\n.gs_init_demo <- function(title = \"MCP Demo Contacts\") {\n  .gs4_auth_once()\n  df0 <- tibble(name = character(), email = character(), city = character())\n  ss <- gs4_create(name = title, sheets = list(contacts = df0))\n  # Вернём удобные поля для LLM\n  meta <- gs4_get(ss)\n  return(jsonlite::toJSON(\n    list(\n      spreadsheet_url = meta$spreadsheet_url,\n      spreadsheet_id = as_sheets_id(ss)$spreadsheet_id,\n      worksheet = \"contacts\"\n    )\n  )\n  )\n}\n\n# 2) Прочитать верхние n строк -------------------------------------------------\n.gs_read <- function(sheet, worksheet = \"contacts\", range = NULL, n_max = 5) {\n  googlesheets4::gs4_auth(path = Sys.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\", unset = \"\"))\n  res <- suppressMessages(googlesheets4::read_sheet(sheet, sheet = worksheet, range = range, n_max = n_max))\n  return(jsonlite::toJSON(res, pretty = TRUE, auto_unbox = TRUE))\n}\n\n# 3) Добавить одну строку ------------------------------------------------------\n.gs_append_contact <- function(sheet, worksheet = \"data\", name, position = NA, rate) {\n  googlesheets4::gs4_auth(path = Sys.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\", unset = \"\"))\n  df <- tibble(name = name, position = position, rate = rate)\n  googlesheets4::sheet_append(ss = sheet, data = df, sheet = worksheet)\n  return(jsonlite::toJSON(list(ok = TRUE, appended = nrow(df)), pretty = TRUE, auto_unbox = TRUE))\n}\n\n# 4) Прочитать метаданные таблицы ----------------------------------------------\n.gs_info <- function(sheet) {\n  googlesheets4::gs4_auth(path = Sys.getenv(\"GOOGLE_SERVICE_ACCOUNT_JSON\", unset = \"\"))\n  res <- suppressMessages(googlesheets4::gs4_get(sheet))\n  return(jsonlite::toJSON(res, pretty = TRUE, auto_unbox = TRUE))\n}\n\n\n# 5) Проверка значений переменной среды ----------------------------------------\n.gs_show_env <- function(env) {\n  message(\"Переменная среды: \", Sys.getenv(env, unset = \"\"))\n  return(jsonlite::toJSON(Sys.getenv(env, unset = \"\"), pretty = TRUE, auto_unbox = TRUE))\n}\n\n# Описания инструментов (ellmer 0.3.0) -----------------------------------------\n\nTool_gs_init <- tool(\n  .gs_init_demo,\n  name = \"gs_init\",\n  description = \"Create an empty Google Sheet with a 'contacts' worksheet (name, email, city). Returns spreadsheet metadata.\",\n  arguments = list(\n    title = type_string(\"Spreadsheet title\", required = FALSE)\n  )\n)\n\nTool_gs_read <- tool(\n  .gs_read,\n  name = \"gs_read\",\n  description = \"Read a small preview from a Google Sheet. Useful for head/preview, not full export.\",\n  arguments = list(\n    sheet = type_string(\"Google Sheet URL or ID\"),\n    worksheet = type_string(\"Worksheet name\", required = FALSE),\n    range = type_string(\"A1 range like 'A1:D20'\", required = FALSE),\n    n_max = type_integer(\"How many rows to return\", required = FALSE)\n  )\n)\n\nTool_gs_append_contact <- tool(\n  .gs_append_contact,\n  name = \"gs_append_contact\",\n  description = \"Append a single contact row (name, email, optional city) to the Google Sheet.\",\n  arguments = list(\n    sheet = type_string(\"Google Sheet URL or ID\"),\n    worksheet = type_string(\"Worksheet name\"),\n    name      = type_string(\"Employee name\"),\n    position  = type_string(\"Employee job title\", required = FALSE),\n    rate      = type_string(\"Employee salary\")\n  )\n)\n\nTool_gs_info <- tool(\n  .gs_info,\n  name = \"gs_info\",\n  description = \"Read a google sheet metadata info.\",\n  arguments = list(\n    sheet = type_string(\"Google Sheet URL or ID\")\n  )\n)\n\nTool_show_env <- tool(\n  .gs_show_env,\n  name = \"gs_show_env\",\n  description = \"Show environment variable for debug.\",\n  arguments = list(\n    env = type_string(\"Environ variables name\")\n  )\n)\n\nmsum <- function(x) sum(x)\nTool_sum <- tool(\n  msum,\n  name = \"sum\",\n  description = \"Sum of Vector Elements\",\n  arguments = list(\n    x = type_array(type_integer())\n  )\n)\n\nmmax <- function(x) max(x)\nTool_max <- tool(\n  mmax,\n  name = \"max\",\n  description = \"Max value of Vector Elements\",\n  arguments = list(\n    x = type_array(type_integer())\n  )\n)\n\n# MCP сервер должен вернуть список инструментов\nlist(\n  Tool_sum,\n  Tool_max,\n  Tool_gs_init,\n  Tool_gs_read,\n  Tool_gs_append_contact,\n  Tool_gs_info,\n  Tool_show_env\n){\n  \"mcpServers\": {\n    \"r-gsheets\": {\n      \"command\": \"Rscript\",\n      \"args\": [\"-e\", \"mcptools::mcp_server(tools='G:/GS_MCP/gs-tools.R')\"],\n      \"env\": {\n        \"GOOGLE_SERVICE_ACCOUNT_JSON\": \"G:/GS_MCP/service.json\",\n        \"R_LIBS_USER\": \"C:/Users/User/AppData/Local/R/win-library/4.4\"\n      }\n    }\n  }\n}"},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"r-как-mcp-клиент","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3.3 R как MCP клиент","text":"У нас уже есть код и конфиг нашего MCP сервера, теперь осталось разобраться с тем, как подключить его к объекту чата созданного с помощью ellmer::chat_*().В этом разделе мы будем работать с Google таблицами, я заранее создал демо таблицу и расшарил доступ на сервисный аккаунт, который использую в своём MCP клиенте.Для подключения MCP сервера достаточно использовать метод Chat$set_tools():Т.е. в этом примере мы:Создаём объект чата с помощью chat_google_gemini()Подключаем MCP сервер через chat$set_tools(mcp_tools(config = \"gs-tools.json\")), указав путь к созданному ранее конфигу MCP сервера.Далее модель из контекста вашего промпта понимает какие из инструментов MCP сервера ей использовать для выполнения действий.","code":"library(mcptools)\nlibrary(ellmer)\n\n# Создаём объект чата\nchat <- chat_google_gemini(system_prompt = 'Ты помощник по работе с google таблицами')\n\n# Подключаем MCP сервер\nchat$set_tools(mcp_tools(config = \"G:\\GS_MCP\\gs-tools.json\"))\n\n# Используем инструменты из MCP сервера\nchat$chat('Привет, скажи какой список инструментов у тебя есть?')\n\n# Проверяем что переменные окружения успешно считались из конфига MCP сервера\nchat$chat('Покажи мне значение переменной среды GOOGLE_SERVICE_ACCOUNT_JSON')\n\n# Читаем Google таблицу\nchat$chat(\"Посмотри какие метаданные есть в таблице 1MARmNaTbn0OhIoHM7EsNa7hh48VPUpxQN5B1gGYFSD0\")\nchat$chat(\"Теперь у той же таблицы с ключом 1MARmNaTbn0OhIoHM7EsNa7hh48VPUpxQN5B1gGYFSD0 прочитай данные с листа data, что там?\")\nchat$chat(\"Хорошо, а можешь теперь посчитать общую сумму зарплат из поля rate?\")\nchat$chat(\"У кого из сотрудников в таблице самая высокая зарплата?\")\n\n# Запись данных\nchat$chat(\"А теперь запиши в эту же таблицу новую строку, имя Erik, должность CFO, оклад 4250\")"},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"заворачиваем-ai-ассистента-в-интерфейс-чата","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3.4 Заворачиваем AI ассистента в интерфейс чата","text":"Что бы этот урок был максимально автономным ещё раз вспомним о пакете shinychat и обернём наш чат бот в веб интерфейс.Теперь ваш AI ассистент будет иметь графический интерфейс, и уметь работать с Google Таблицами.","code":"\nlibrary(shiny)\nlibrary(shinychat)\nlibrary(mcptools)\n\nui <- bslib::page_fillable(\n  chat_ui(\n    id = \"chat\",\n    messages = \"Привет, я могу помочь в работе с Google Таблицами, что надо сделать?\"\n  ),\n  fillable_mobile = TRUE\n)\n\nserver <- function(input, output, session) {\n  \n  # Создаём объект чата\n  chat <- ellmer::chat_google_gemini(\n    system_prompt = 'Ты помощник по работе с google таблицами'\n  )\n  \n  # Подключаем MCP сервер\n  chat$set_tools(mcp_tools(config = r\"(G:\\GS_MCP\\gs-tools.json)\"))\n  \n  observeEvent(input$chat_user_input, {\n    stream <- chat$stream_async(input$chat_user_input)\n    chat_append(\"chat\", stream)\n  })\n}\n\nshinyApp(ui, server)"},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"подключаем-mcp-сервер-к-claude-desktop","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.3.5 Подключаем MCP сервер к Claude Desktop","text":"Написанный на R MCP сервер можно подключать не только к ellmer, но и к другим клиентам, например Claude Desktop.Для этого в Claude Desktop перейдите в Settings / Developer / Edit config, и добавьте созданный ранее JSON конфиг для подключения нашего MCP сервера.После перезапуска Claude Desktop вы сможете использовать MCP сервер:","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"заключение-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.4 Заключение","text":"Теперь ваш ИИ-ассистент умеет действовать: он может создавать таблицы, считать зарплаты и менять данные по вашей команде. Вы построили универсальный мост между логикой R и интеллектом LLM.Однако возможности ассистента все еще ограничены теми данными, которые мы передаем в промпте или которые он может достать через функции. А что, если у вас тысячи PDF-отчетов, гигабайты технической документации или огромная база знаний, которую невозможно “скормить” модели целиком? В следующей главе мы разберем технологию RAG (Retrieval-Augmented Generation). Мы научим нашего бота самостоятельно искать нужную информацию в огромных массивах ваших документов и давать ответы, основываясь только на проверенных фактах из вашей базы знани","code":""},{"path":"язык-r-как-mcp-сервер-и-mcp-клиент.html","id":"вопросы-для-самопроверки-2","chapter":"Глава 3 Язык R как MCP сервер и MCP клиент","heading":"3.5 Вопросы для самопроверки","text":"В чем принципиальное различие между обычным вызовом функции (Function Calling) и использованием MCP-сервера?\n\nFunction Calling обычно настраивается внутри одного приложения для одной модели. MCP-сервер — это универсальный стандарт: один раз написав сервер на R, вы можете подключить его к разным клиентам (Claude Desktop, IDE, Shiny, сторонние боты) без переписывания кода.\nПочему функции внутри MCP-сервера на R должны возвращать именно JSON, а не обычные объекты R (data.frame, list)?\n\nПротокол MCP основан на JSON-RPC. Чтобы языковая модель (которая “общается” через текстовые токены) и клиент смогли корректно интерпретировать данные, они должны быть сериализованы в стандартный формат JSON.\nДля чего в конфиге MCP-сервера на Windows часто приходится вручную прописывать переменную R_LIBS_USER?\n\nПри запуске через Rscript из внешнего приложения (например, Claude Desktop) системное окружение может отличаться от вашего привычного RStudio. Без этой переменной R может не найти установленные пакеты (googlesheets4, ellmer и др.) в вашей пользовательской библиотеке.\nКакую роль играет описание (description) в функции ellmer::tool()?\n\nЭто “инструкция по применению” для ИИ. Именно по этому описанию модель понимает, какую задачу решает функция и в какой ситуации её нужно вызвать. Чем точнее описание, тем реже модель будет ошибаться.\nКак передать в MCP-сервер секретные данные (например, ключи доступа), не вписывая их в основной код функций?\n\nЛучше всего передавать их через блок env в JSON-конфигурации сервера. В самом R-коде эти значения будут доступны через Sys.getenv(). Это позволяет разделять логику кода и конфиденциальные данные.\n","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"rag-подключаем-llm-модель-к-собственной-базе-знаний","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","text":"В предыдущей главе мы научили нашего ассистента пользоваться внешними инструментами через MCP. Но что делать, если задача требует не выполнения функции, а глубокого знания специфической документации? Надеяться на общую эрудицию LLM опасно — она может “галлюцинировать” или просто не знать о существовании ваших внутренних регламентов и свежих учебников.В этой главе мы освоим технологию RAG (Retrieval-Augmented Generation). Мы создадим собственную базу знаний на основе учебника по Telegram-ботам, научим систему превращать текст в математические векторы и сохранять их в высокопроизводительное хранилище DuckDB. В итоге мы получим экспертную систему, которая отвечает на вопросы строго по вашим документам, предоставляя ссылки на первоисточники","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"видео-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.1 Видео","text":"","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"тайм-коды-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.1.1 Тайм коды","text":"00:00 — О чём это видео01:54 — Концепция RAG: теория и применение06:02 — Настройка переменных среды для API ключей07:31 — Подготовка и загрузка документов (пакет ragnar)10:44 — Чанкинг, эмбеддинг и запись в векторное хранилище22:24 — Обзор инструмента Ragnar Store Inspector24:05 — Гибридный поиск: сравнение VSS и BM2531:13 — Подключение LLM к базе знаний (Retrieval Tool)35:08 — Разработка UI интерфейса в shinychat38:55 — Резюме пройденного материала41:22 — Заключение","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"презентация-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.2 Презентация","text":"","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"конспект-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3 Конспект","text":"","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"что-такое-rag-простыми-словами","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.1 Что такое RAG простыми словами","text":"Если обычная LLM (например, Gemini или ChatGPT) — это очень умный студент, который прочитал весь интернет, но ничего не знает о ваших личных файлах, то RAG — это тот же студент, которому разрешили пользоваться библиотекой ваших документов во время экзамена.Вместо того чтобы пытаться запихнуть всю документацию вашей компании в системный промпт (где место ограничено и это стоит дорого), мы учим модель:Сначала найти нужные куски текста в базе данных.Затем прочитать их и на их основе составить ответ.","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"обзор-рабочего-процесса-rag","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.2 Обзор рабочего процесса RAG","text":"В этом уроке мы с вами будем разбираться с пакетом ragnar, о котором ранее в этом курсе ещё не упоминалось. ragnar имеет в арсенале весь необходимый функционал для реализации RAG подхода.Рабочий процесс построения RAG:Обработка документов, это первый этап на котором вам необходимо привести ваши документы к пригодному для дальнейшей работы формату. В ragnar для этого есть 2 функции:read_as_markdown() - Читает документы: pdf, power point, word, excel, zip, youtube видео и другие, и приводит их к markdown разметке.ragnar_find_links() - Ищет все ссылки на заданной веб-странице, для удобной подготовки любого сайта как будущей базы данных.Разбивка документов на фрагменты, этот этап реализуется функцией markdown_chunk(), которая является полнофункциональным инструментом для разделения текста на фрагменты.Разбивка документов на фрагменты, этот этап реализуется функцией markdown_chunk(), которая является полнофункциональным инструментом для разделения текста на фрагменты.Расширение контекста фрагментов, опциональный шаг, позволяющий добавить в каждый фрагмент текста дополнительный контекст, например заголовки. Реализуется функцией markdown_chunk().Расширение контекста фрагментов, опциональный шаг, позволяющий добавить в каждый фрагмент текста дополнительный контекст, например заголовки. Реализуется функцией markdown_chunk().Эмбединг, процесс векторного кодирования каждого отдельного фрагмента текста, реализуется семейством функций embed_*().Эмбединг, процесс векторного кодирования каждого отдельного фрагмента текста, реализуется семейством функций embed_*().Хранение базы знаний, зашифрованные фрагменты текста необходимо записать в хранилище, для этого есть несколько функций:Хранение базы знаний, зашифрованные фрагменты текста необходимо записать в хранилище, для этого есть несколько функций:ragnar_store_create() - создание хранилище, по умолчанию используется DuckDBragnar_store_connect() - подключение к хранилищуragnar_store_insert() - запись данных в хранилищеПоиск и извлечение релевантных фрагментов, для этого этапа в ragnar также имеется семейство функций:ragnar_retrieve() - высокоуровневая функция, которая совмещает в себе vss и bm25 поиск, а так же обработку полученных результатов.ragnar_retrieve_vss() - реализует vss поиск по базе знаний.ragnar_retrieve_bm25() - реализует полнотекстовый поиск по базе знаний.chunks_deoverlap() - объединяет полученные поиском фрагменты учитывая то, что они могут перекрывать друг друга.Подключаем чат к базе знаний, финальный этап, на котором мы подключаем базу знаний к чату с помощью функции ragnar_register_tool_retrieve(chat, store).Далее каждый шаг разберём более подробно на реальном примере.","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"обработка-документов-фрагментирование-эмбединг-и-запись-в-хранилище","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.3 Обработка документов: фрагментирование, эмбединг и запись в хранилище","text":"В качестве базы знаний я буду использовать свой учебник по разработке telegram ботов. Для начала нам необходимо получить ссылки на все главы учебника:","code":"\nlibrary(ragnar)\nlibrary(tidyverse)\nlibrary(ellmer)\n\n# 1. Создание базы знаний -------------------------------------------------\n\n# задаём URL документации\nbase_url <- \"https://selesnow.github.io/build_telegram_bot_using_r/\"\n\n# считываем все ссылки исключая не нужные\npages <- ragnar_find_links(base_url) %>% \n         .[stringr::str_detect(., 'https://selesnow.github.io/build_telegram_bot_using_r')]"},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"разбивка-документов-на-фрагменты","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.4 Разбивка документов на фрагменты","text":"На этом этапе у нас есть вектор из URL на каждую главу учебника, который будет использоваться в качестве базы данных. Теперь каждую главу отдельно необходимо обработать, и разбить её дополнительно на фрагменты, но перед этим отвлечёмся немного от нашего реального примера, и рассмотрим примеры использования аргументов функции markdown_chunk().Аргументы:\n* md - MarkdownDocument, или текст в Markdown разметке.\n* target_size - Целевой размер фрагмента в символах. По умолчанию: 1600 (приблизительно 400 токенов или 1 страница текста). Фактический размер блока может отличаться от целевого значения до 2 * max_snap_dist. Если установлено значение NULL, NA или Inf используется с segment_by_heading_levels, размер блока неограничен, и каждый блок соответствует сегменту.\n* target_overlap - Числовое значение в [0, 1). Доля желаемого перекрытия между последовательными фрагментами.\n* max_snap_dist - Максимальное расстояние (в символах), на которое может переместиться точка разреза, чтобы достичь семантической границы.\n* segment_by_heading_levels - Целочисленный вектор с возможными значениями 1:6. Заголовки на этих уровнях рассматриваются как границы сегментов; разбиение на фрагменты выполняется независимо для каждого сегмента.\n* context - Добавить столбец context, содержащий заголовки Markdown, находящиеся в области видимости, в начале каждого фрагмента кода.\n* text - Если значение равно TRUE, включить textстолбец с содержимым фрагмента.Что бы лучше понять работу каждого отдельного аргумента можно обратиться к примерам из справки:Теперь вернёмся к нашему примеру, у нас есть вектор с ссылкой на каждую главу учебника по разработке teelgram ботов, наша задача - разбить по очереди каждую главу учебника на фрагменты, векторизировать каждый фрагмент и записать в хранилище.При создании хранилища с помощью ragnar_store_create() вам сразу необходимо передать в аргумент embed функцию реализующую эмбединг. Эмбеддинг (embedding) - это числовой вектор, который как можно компактнее и осмысленно кодирует смысл фрагмента текста (слова/предложения/параграфа/страницы). В RAG он превращает текст в пространство, где «похожесть смыслов» измеряется расстоянием между векторами — и на этом строится извлечение релевантных фрагментов. На данный момент в ragnar довольно большой набор функций для эмбединга:embed_azure_openai()embed_bedrock()embed_databricks()embed_google_gemini()embed_google_vertex()embed_lm_studio()embed_ollama()embed_openai()embed_snowflake()Все облачные модели требуют передачи API ключа, для тестов, как я говорил в первой главе, вы можете использовать бесплатный ключ к Gemini API, более подробно об этом я рассказывал в разделе Генерация API ключа для работы с LLM, единственная разница в том, что авторы ragnar используют своё название переменной среды для хранения API ключа, вместо GOOGLE_API_KEY вам необходимо передать ваш API ключ в переменную GEMINI_API_KEY.После создания хранилища в цикле мы собираем обрабатываем каждую главу учебника, разбиваем на фрагменты, и записываем в хранилище, при записи в хранилище автоматически применяется эмбединг, т.к. функцию для эмбединга мы задали на прошлом шаге при создании хранилища.Далее, что бы корректно работал поиск по базе знаний необходимо построить индекс с помощью ragnar_store_build_index().Каждый раз при добавлении новой информации в базу знаний индекс необходимо перестраивать.В ragnar отдельно встроен функционал Ragnar Store Inspector, запустить его можно с помощью ragnar_store_inspect(store). С его помощью вы можете выполнять через графический интерфейс поиск по вашему хранилищу, это поможет вам определиться с тем, успешно вы разбили ваш исходный документ на чанки, или нет.Вы задаёте запрос, далее все фрагменты в правой части интерфейса сортируются по релевантности в порядке убывания, т.е. наиболее релевантный вашему запросу чанк должен быть самым первым. Если вы получается фрагменты релевантные запросу значит вы достаточно качествено выполнили разделения документов на фрагменты, в противном случае вам стоит вернуться к разделу Обработка документов: фрагментирование, эмбединг и запись в хранилище, и переосмыслить фрагментирования (чанкинга), поигравшись с аргументами функции markdown_chunk().Совет: Используйте инспектор не только для проверки поиска, но и для отладки чанкинга. Если вы видите в результатах обрывки фраз или нечитаемые куски кода — значит, параметры target_size или max_snap_dist нужно подкорректировать.","code":"\nlibrary(ragnar)\n\nmd <- \"\n# Title\n\n## Section 1\n\nSome text that is long enough to be chunked.\n\nA second paragraph to make the text even longer.\n\n## Section 2\n\nMore text here.\n\n### Section 2.1\n\nSome text under a level three heading.\n\n#### Section 2.1.1\n\nSome text under a level four heading.\n\n## Section 3\n\nEven more text here.\n\"\n\nmarkdown_chunk(md, target_size = 40)\nmarkdown_chunk(md, target_size = 40, target_overlap = 0)\nmarkdown_chunk(md, target_size = NA, segment_by_heading_levels = c(1, 2))\nmarkdown_chunk(md, target_size = 40, max_snap_dist = 100)\n# задём путь хранилища\nstore_location <- \"tgbot_rag.duckdb\"\n\n# создаём хранилище\nstore <- ragnar_store_create(\n  store_location,\n  embed = \\(x) ragnar::embed_google_gemini(x)\n)\n# разбиваем каждую отдельную страницу на чанки\n# и записываем в хранилище\nfor (page in pages) {\n  message(\"ingesting: \", page)\n  chunks <- page |> read_as_markdown() |> markdown_chunk()\n  ragnar_store_insert(store, chunks)\n}\n# строим индекс для поиска\nragnar_store_build_index(store)"},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"поиск-по-хранилищу","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.5 Поиск по хранилищу","text":"Программно тоже можно осуществлять поиск по хранилищу:В объекте relevant_chunks в результате будут хранится наиболее релевантные вашему запросу чанку, с дополнительным контекстом:Я уже писал выше, что ragnar совмещает два типа поиска:VSS - Поиск по смыслу. Ищем не совпадение слов, а близость смысла. Удобно, например, для поиска нужной функций по её описанию.BM25 - Поиск по ключевым словам. Ищем буквальное совпадение текста. Например, вы знаете точное название нужной функции, и хотите найти её описание.","code":"\n# 2. Обращение к базе знаний ----------------------------------------------\n\n# подключение к базе знаний\nstore <- ragnar_store_connect(store_location, read_only = TRUE)\n\ntext <- \"Можно ли хранить токен telegram бота в переменной среды?\"\n\n# запрос информации из базы знаний\nrelevant_chunks <- ragnar_retrieve(store, text)> relevant_chunks\n# A tibble: 3 × 9\n  origin                                                 doc_id chunk_id start   end cosine_distance bm25  context text \n  <chr>                                                   <int> <list>   <int> <int> <list>          <lis> <chr>   <chr>\n1 https://selesnow.github.io/build_telegram_bot_using_r…     10 <int>    23266 24775 <dbl [1]>       <dbl> \"# Гла… \"Дал…\n2 https://selesnow.github.io/build_telegram_bot_using_r…     10 <int>    39143 40839 <dbl [1]>       <dbl> \"# Гла… \"###…\n3 https://selesnow.github.io/build_telegram_bot_using_r…     12 <int>     4767  7211 <dbl [2]>       <dbl> \"# Гла… \"## …"},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"создаём-интерфейс-чата","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.3.6 Создаём интерфейс чата","text":"Итак, давайте ещё раз резумируем весь процесс создания хранилища с базой знаний, именно её мы далее будем использовать при создании интерфейса чата.Подготовка хранилища выполняется один раз, далее вы можете дополнять базу знаний, но не забывайте что при каждом изменении надо повторно строить индекс.Далее, с помощью пакета shinychat, знакомого нам по нескольким прошлым главам мы можем создать графический интерфейс для чата.Подключение модели к базе знаний реализуется там же где и инициализация чата, т.е. в серверной части приложения, там же вы добавляете в объект chat инструмент get_data_from_knowledge_store, позволяющей модели осуществлять поиск по вашей базе знаний.Т.к. я в системном промпте прописал инструкции о том, что модель в конце ответа обязательно должна предоставить фрагменты базы знаний, которые были использованы при формировании ответа, то в конце ответа мы видим следующее:","code":"\nlibrary(ragnar)\nlibrary(tidyverse)\nlibrary(ellmer)\n\n# 1. Создание базы знаний -------------------------------------------------\n\n# задаём URL документации\nbase_url <- \"https://selesnow.github.io/build_telegram_bot_using_r/\"\n\n# считываем все ссылки исключая не нужные\npages <- ragnar_find_links(base_url) %>% \n         .[stringr::str_detect(., 'https://selesnow.github.io/build_telegram_bot_using_r')]\n\n# задём путь хранилища\nstore_location <- \"tgbot_rag.duckdb\"\n\n# создаём хранилище\nstore <- ragnar_store_create(\n  store_location,\n  embed = \\(x) ragnar::embed_google_gemini(x)\n)\n\n# разбиваем каждую отдельную страницу на чанки\n# и записываем в хранилище\nfor (page in pages) {\n  message(\"ingesting: \", page)\n  chunks <- page |> read_as_markdown() |> markdown_chunk()\n  ragnar_store_insert(store, chunks)\n}\n\n# строим индекс для поиска\nragnar_store_build_index(store)\n\n# инструмент проверки хранилища\nragnar_store_inspect(store)\nlibrary(shiny)\nlibrary(shinychat)\n\nui <- bslib::page_fillable(\n  chat_ui(\n    id = \"chat\",\n    messages = \"**Привет!** я помогаю в разработке telegram ботов на языке R. Чем могу тебе помочь?\"\n  ),\n  fillable_mobile = TRUE\n)\n\nserver <- function(input, output, session) {\n  \n  # подключаем базу знаний к ellmer чату\n  chat <- ellmer::chat_google_gemini(\n    system_prompt =  stringr::str_squish(\n      \"Ты помощник по разработке telegram ботов на языке R. \n       Для формирования каждого ответа сначала ищи информацию в своей базе знаний используя инструмент get_data_from_knowledge_store, \n       т.е. предупреждай, что начинаешь поиск по базе знаний, используй инструмент get_data_from_knowledge_store и только потом отвечай \n       с учётом полученной из базы знаний информации, это обязательное условие для формирования ответов.\n       Процитируй или перефразируй отрывки, четко отличая свои слова от слов источника. \n       Предоставь рабочую ссылку на каждый цитируемый источник, а также любые дополнительные соответствующие ссылки.\n       Так же всегда добавляй в ответ дополнительный контекст чанков которые использовал для формирования самого ответа, заголовки частей из которых был получен используемый чанк в формате списка с ссылками, где текст бери из поля context используемого чанка, а сама ссылка поля origin используемого чанка.\n       Т.е. в конце сообщения ты обязательно должен вывести информацию про используемые для ответа чанки из базы знаний.\n       Если ты не смог найти ни один чанк в базе знаний то в ответе просто скажи, что по вашему запросу я не сумел найти ничего в своей базе знаний, т.е. сам никогда ничего не придумывай, это важно!\n      \"\n    ),\n    model = 'gemini-2.0-flash',  \n    echo  = 'none'\n  )\n  \n  # подключаемся к хранилищу с базой знаний\n  store <- ragnar_store_connect(\"tgbot_rag.duckdb\", read_only = TRUE)\n  # добавляем модели инструмент поиска по базе знаний\n  ragnar_register_tool_retrieve(chat, store, top_k = 10, name = 'get_data_from_knowledge_store', title = 'knowledge_store')\n  \n  observeEvent(input$chat_user_input, {\n    stream <- chat$stream_async(input$chat_user_input)\n    chat_append(\"chat\", stream)\n  })\n}\n\nshinyApp(ui, server)"},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"заключение-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.4 Заключение","text":"Поздравляем! Вы прошли путь от создания простого чат-бота до построения сложной экспертной системы на языке R. Теперь ваш ассистент не просто владеет инструментами, но и обладает “памятью”, основанной на ваших документах. Это финальная точка нашего курса, где воедино сошлись Shiny, Telegram, MCP и векторные базы данных.Однако мир ИИ не стоит на месте. Теперь, когда вы освоили базу, вы можете масштабировать свои решения: подключать облачные векторные базы данных для работы с терабайтами данных или использовать локальные модели через Ollama для полной конфиденциальности. Главное, что теперь у вас есть надежный стек технологий на языке R для решения любых бизнес-задач с помощью искусственного интеллекта. Удачи в ваших проектах!","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"вопросы-для-самопроверки-3","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.5 Вопросы для самопроверки","text":"","code":""},{"path":"rag-подключаем-llm-модель-к-собственной-базе-знаний.html","id":"вопросы-для-самопроверки-4","chapter":"Глава 4 RAG: Подключаем LLM модель к собственной базе знаний","heading":"4.6 Вопросы для самопроверки","text":"Почему для реализации RAG-подхода недостаточно просто скопировать весь текст учебника в системный промпт?\n\nУ каждой модели есть ограничение на размер контекстного окна. Кроме того, передача огромных массивов текста в каждом запросе сильно замедляет работу и делает каждый вопрос пользователя очень дорогим. RAG позволяет передавать только те 3-5 фрагментов, которые действительно нужны для ответа.\nВ чем разница между поиском VSS и поиском BM25, которые комбинирует функция ragnar_retrieve()?\n\nVSS (Vector Semantic Search) ищет похожие по смыслу фрагменты, даже если слова не совпадают. BM25 — это классический полнотекстовый поиск, который находит точные вхождения слов. Их комбинация (гибридный поиск) обеспечивает максимальную точность.\nЗачем нужно устанавливать target_overlap (перекрытие) при разбиении текста на чанки?\n\nПерекрытие гарантирует, что важный контекст не будет “разрезан” пополам. Если предложение начинается в одном чанке и заканчивается в другом, модель сможет понять смысл фрагмента целиком, имея доступ к контексту из соседнего чанка.\nПочему после добавления новых документов в хранилище обязательно нужно вызывать ragnar_store_build_index()?\n\nИндекс — это специальная структура данных, которая позволяет искать похожие векторы за миллисекунды. Без перестроения индекса новые данные будут лежать в таблице, но поисковые алгоритмы не смогут их найти.\nКак системный промпт защищает от “галлюцинаций” модели при работе с базой знаний?\n\nС помощью инструкции «если ты не смог найти информацию в базе знаний — так и скажи». Это запрещает модели использовать свои общие знания из интернета и заставляет её опираться только на предоставленные факты.\n","code":""},{"path":"заключение-4.html","id":"заключение-4","chapter":"Глава 5 Заключение","heading":"Глава 5 Заключение","text":"Поздравляем! Вы завершили основной блок курса и прошли путь от простого чата до создания сложных интеллектуальных систем. Сегодня AI — это не просто «хайп», а обязательный навык для современного аналитика и разработчика. Вы получили в руки стек технологий, который делает вас на голову выше на рынке труда и позволяет автоматизировать задачи, на которые раньше уходили недели.","code":""},{"path":"заключение-4.html","id":"что-мы-изучили-и-внедрили","chapter":"Глава 5 Заключение","heading":"5.1 Что мы изучили и внедрили:","text":"Урок 1: Фундамент и первый чат. Мы научились «приручать» LLM через пакет ellmer, извлекать структурированные данные из хаоса и упаковывать всё это в современные интерфейсы shinychat.Урок 2: Интеллект в кармане. Мы перенесли AI в Telegram, решив критически важную задачу сохранения контекста диалогов для множества пользователей одновременно.Урок 3: Протокол MCP — ИИ с доступом к инструментам. Мы освоили Model Context Protocol, превратив R в сервер, который дает моделям (включая Claude Desktop) доступ к вашим локальным функциям и данным.Урок 4: RAG — ИИ с вашими знаниями. Мы построили базу знаний на векторной базе данных DuckDB, научив модель отвечать строго по вашим документам, исключая галлюцинации.Ваш главный результат: Вы научились строить не просто «игрушки», а прикладные инструменты, которые работают на базе ваших данных, ваших функций и ваших бизнес-процессов.","code":""},{"path":"заключение-4.html","id":"куда-развиваться-дальше","chapter":"Глава 5 Заключение","heading":"5.2 Куда развиваться дальше?","text":"Мир генеративного AI меняется каждую неделю. Чтобы оставаться на острие, пробуйте следующее:Комбинируйте знания: Создайте Telegram-бота, который через MCP обращается к вашей базе знаний (RAG) и выдает готовые отчеты.Экспериментируйте с моделями: Пробуйте локальные модели через Ollama, если работаете с чувствительными данными.Оптимизируйте промпты: Изучайте техники Prompt Engineering, чтобы ответы моделей становились еще точнее и дешевле.","code":""},{"path":"заключение-4.html","id":"это-только-начало","chapter":"Глава 5 Заключение","heading":"5.3 Это только начало","text":"Курс не является законченным памятником — он живой. Я постоянно работаю над ним, дополняю текущие материалы и готовлю новые уроки. Совсем скоро мы разберем продвинутую кастомизацию интерфейсов, автономных агентов и более сложные сценарии интеграции.Чтобы не пропустить выход новых глав и всегда быть в курсе последних обновлений в мире автоматизации маркетинга и AI на языке R, подписывайтесь на мои ресурсы:YouTube канал: R4marketing — здесь выходят все видео-уроки и практические кейсы.Telegram канал: R4marketing — оперативные новости, анонсы обновлений курса и живое общение.Спасибо, что прошли этот путь со мной. До встречи в новых уроках!\n","code":""}]
